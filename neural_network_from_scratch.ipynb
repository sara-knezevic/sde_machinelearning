{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture):\n",
    "    # random seed initiation\n",
    "    np.random.seed(42)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for id_x, layer in enumerate(nn_architecture):\n",
    "        # count network layers from 1\n",
    "        layer_id = id_x + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_id)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_id)] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig) # calculating derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        \n",
    "        # calculation of activation for the current layer\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "        A_curr = sigmoid(Z_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "        \n",
    "    # return predicted & saved values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        m_previous = A_prev.shape[1]\n",
    "\n",
    "        # calculation of the activation function derivative\n",
    "        dZ_curr = sigmoid_backward(dA_curr, Z_curr)\n",
    "\n",
    "        # derivative of the weights matrix \n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / m_previous\n",
    "        # derivative of the bias vector\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m_previous\n",
    "        # derivative of the previous layer activated matrix\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.33] = 1\n",
    "    probs_[probs_ <= 0.33] = 0\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        Y_hat, cashe = forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # calculating gradient\n",
    "        grads_values = backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        # updating model state\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if (i % 50 == 0):\n",
    "            print(\"Iteration: {} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            \n",
    "    return params_values, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 20, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 20, \"output_dim\": 20, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 20, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "X = (X - X.mean()) / X.std()\n",
    "y = (data.target != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - cost: 0.78157 - accuracy: 0.66667\n",
      "Iteration: 50 - cost: 0.66789 - accuracy: 0.66667\n",
      "Iteration: 100 - cost: 0.64373 - accuracy: 0.66667\n",
      "Iteration: 150 - cost: 0.63829 - accuracy: 0.66667\n",
      "Iteration: 200 - cost: 0.63699 - accuracy: 0.66667\n",
      "Iteration: 250 - cost: 0.63665 - accuracy: 0.66667\n",
      "Iteration: 300 - cost: 0.63654 - accuracy: 0.66667\n",
      "Iteration: 350 - cost: 0.63648 - accuracy: 0.66667\n",
      "Iteration: 400 - cost: 0.63644 - accuracy: 0.66667\n",
      "Iteration: 450 - cost: 0.63640 - accuracy: 0.66667\n",
      "Iteration: 500 - cost: 0.63637 - accuracy: 0.66667\n",
      "Iteration: 550 - cost: 0.63633 - accuracy: 0.66667\n",
      "Iteration: 600 - cost: 0.63630 - accuracy: 0.66667\n",
      "Iteration: 650 - cost: 0.63626 - accuracy: 0.66667\n",
      "Iteration: 700 - cost: 0.63622 - accuracy: 0.66667\n",
      "Iteration: 750 - cost: 0.63619 - accuracy: 0.66667\n",
      "Iteration: 800 - cost: 0.63615 - accuracy: 0.66667\n",
      "Iteration: 850 - cost: 0.63611 - accuracy: 0.66667\n",
      "Iteration: 900 - cost: 0.63608 - accuracy: 0.66667\n",
      "Iteration: 950 - cost: 0.63604 - accuracy: 0.66667\n",
      "Iteration: 1000 - cost: 0.63600 - accuracy: 0.66667\n",
      "Iteration: 1050 - cost: 0.63596 - accuracy: 0.66667\n",
      "Iteration: 1100 - cost: 0.63592 - accuracy: 0.66667\n",
      "Iteration: 1150 - cost: 0.63589 - accuracy: 0.66667\n",
      "Iteration: 1200 - cost: 0.63585 - accuracy: 0.66667\n",
      "Iteration: 1250 - cost: 0.63581 - accuracy: 0.66667\n",
      "Iteration: 1300 - cost: 0.63577 - accuracy: 0.66667\n",
      "Iteration: 1350 - cost: 0.63573 - accuracy: 0.66667\n",
      "Iteration: 1400 - cost: 0.63569 - accuracy: 0.66667\n",
      "Iteration: 1450 - cost: 0.63565 - accuracy: 0.66667\n",
      "Iteration: 1500 - cost: 0.63561 - accuracy: 0.66667\n",
      "Iteration: 1550 - cost: 0.63556 - accuracy: 0.66667\n",
      "Iteration: 1600 - cost: 0.63552 - accuracy: 0.66667\n",
      "Iteration: 1650 - cost: 0.63548 - accuracy: 0.66667\n",
      "Iteration: 1700 - cost: 0.63544 - accuracy: 0.66667\n",
      "Iteration: 1750 - cost: 0.63539 - accuracy: 0.66667\n",
      "Iteration: 1800 - cost: 0.63535 - accuracy: 0.66667\n",
      "Iteration: 1850 - cost: 0.63530 - accuracy: 0.66667\n",
      "Iteration: 1900 - cost: 0.63525 - accuracy: 0.66667\n",
      "Iteration: 1950 - cost: 0.63521 - accuracy: 0.66667\n",
      "Iteration: 2000 - cost: 0.63516 - accuracy: 0.66667\n",
      "Iteration: 2050 - cost: 0.63511 - accuracy: 0.66667\n",
      "Iteration: 2100 - cost: 0.63506 - accuracy: 0.66667\n",
      "Iteration: 2150 - cost: 0.63501 - accuracy: 0.66667\n",
      "Iteration: 2200 - cost: 0.63496 - accuracy: 0.66667\n",
      "Iteration: 2250 - cost: 0.63491 - accuracy: 0.66667\n",
      "Iteration: 2300 - cost: 0.63486 - accuracy: 0.66667\n",
      "Iteration: 2350 - cost: 0.63480 - accuracy: 0.66667\n",
      "Iteration: 2400 - cost: 0.63475 - accuracy: 0.66667\n",
      "Iteration: 2450 - cost: 0.63469 - accuracy: 0.66667\n",
      "Iteration: 2500 - cost: 0.63464 - accuracy: 0.66667\n",
      "Iteration: 2550 - cost: 0.63458 - accuracy: 0.66667\n",
      "Iteration: 2600 - cost: 0.63452 - accuracy: 0.66667\n",
      "Iteration: 2650 - cost: 0.63446 - accuracy: 0.66667\n",
      "Iteration: 2700 - cost: 0.63440 - accuracy: 0.66667\n",
      "Iteration: 2750 - cost: 0.63434 - accuracy: 0.66667\n",
      "Iteration: 2800 - cost: 0.63427 - accuracy: 0.66667\n",
      "Iteration: 2850 - cost: 0.63421 - accuracy: 0.66667\n",
      "Iteration: 2900 - cost: 0.63414 - accuracy: 0.66667\n",
      "Iteration: 2950 - cost: 0.63407 - accuracy: 0.66667\n",
      "Iteration: 3000 - cost: 0.63400 - accuracy: 0.66667\n",
      "Iteration: 3050 - cost: 0.63393 - accuracy: 0.66667\n",
      "Iteration: 3100 - cost: 0.63385 - accuracy: 0.66667\n",
      "Iteration: 3150 - cost: 0.63378 - accuracy: 0.66667\n",
      "Iteration: 3200 - cost: 0.63370 - accuracy: 0.66667\n",
      "Iteration: 3250 - cost: 0.63362 - accuracy: 0.66667\n",
      "Iteration: 3300 - cost: 0.63354 - accuracy: 0.66667\n",
      "Iteration: 3350 - cost: 0.63346 - accuracy: 0.66667\n",
      "Iteration: 3400 - cost: 0.63337 - accuracy: 0.66667\n",
      "Iteration: 3450 - cost: 0.63329 - accuracy: 0.66667\n",
      "Iteration: 3500 - cost: 0.63320 - accuracy: 0.66667\n",
      "Iteration: 3550 - cost: 0.63311 - accuracy: 0.66667\n",
      "Iteration: 3600 - cost: 0.63301 - accuracy: 0.66667\n",
      "Iteration: 3650 - cost: 0.63292 - accuracy: 0.66667\n",
      "Iteration: 3700 - cost: 0.63282 - accuracy: 0.66667\n",
      "Iteration: 3750 - cost: 0.63271 - accuracy: 0.66667\n",
      "Iteration: 3800 - cost: 0.63261 - accuracy: 0.66667\n",
      "Iteration: 3850 - cost: 0.63250 - accuracy: 0.66667\n",
      "Iteration: 3900 - cost: 0.63239 - accuracy: 0.66667\n",
      "Iteration: 3950 - cost: 0.63228 - accuracy: 0.66667\n",
      "Iteration: 4000 - cost: 0.63216 - accuracy: 0.66667\n",
      "Iteration: 4050 - cost: 0.63204 - accuracy: 0.66667\n",
      "Iteration: 4100 - cost: 0.63192 - accuracy: 0.66667\n",
      "Iteration: 4150 - cost: 0.63179 - accuracy: 0.66667\n",
      "Iteration: 4200 - cost: 0.63166 - accuracy: 0.66667\n",
      "Iteration: 4250 - cost: 0.63152 - accuracy: 0.66667\n",
      "Iteration: 4300 - cost: 0.63138 - accuracy: 0.66667\n",
      "Iteration: 4350 - cost: 0.63124 - accuracy: 0.66667\n",
      "Iteration: 4400 - cost: 0.63109 - accuracy: 0.66667\n",
      "Iteration: 4450 - cost: 0.63094 - accuracy: 0.66667\n",
      "Iteration: 4500 - cost: 0.63078 - accuracy: 0.66667\n",
      "Iteration: 4550 - cost: 0.63062 - accuracy: 0.66667\n",
      "Iteration: 4600 - cost: 0.63045 - accuracy: 0.66667\n",
      "Iteration: 4650 - cost: 0.63028 - accuracy: 0.66667\n",
      "Iteration: 4700 - cost: 0.63010 - accuracy: 0.66667\n",
      "Iteration: 4750 - cost: 0.62992 - accuracy: 0.66667\n",
      "Iteration: 4800 - cost: 0.62973 - accuracy: 0.66667\n",
      "Iteration: 4850 - cost: 0.62953 - accuracy: 0.66667\n",
      "Iteration: 4900 - cost: 0.62933 - accuracy: 0.66667\n",
      "Iteration: 4950 - cost: 0.62912 - accuracy: 0.66667\n",
      "Iteration: 5000 - cost: 0.62890 - accuracy: 0.66667\n",
      "Iteration: 5050 - cost: 0.62867 - accuracy: 0.66667\n",
      "Iteration: 5100 - cost: 0.62844 - accuracy: 0.66667\n",
      "Iteration: 5150 - cost: 0.62820 - accuracy: 0.66667\n",
      "Iteration: 5200 - cost: 0.62795 - accuracy: 0.66667\n",
      "Iteration: 5250 - cost: 0.62769 - accuracy: 0.66667\n",
      "Iteration: 5300 - cost: 0.62742 - accuracy: 0.66667\n",
      "Iteration: 5350 - cost: 0.62714 - accuracy: 0.66667\n",
      "Iteration: 5400 - cost: 0.62686 - accuracy: 0.66667\n",
      "Iteration: 5450 - cost: 0.62656 - accuracy: 0.66667\n",
      "Iteration: 5500 - cost: 0.62625 - accuracy: 0.66667\n",
      "Iteration: 5550 - cost: 0.62593 - accuracy: 0.66667\n",
      "Iteration: 5600 - cost: 0.62559 - accuracy: 0.66667\n",
      "Iteration: 5650 - cost: 0.62525 - accuracy: 0.66667\n",
      "Iteration: 5700 - cost: 0.62489 - accuracy: 0.66667\n",
      "Iteration: 5750 - cost: 0.62451 - accuracy: 0.66667\n",
      "Iteration: 5800 - cost: 0.62412 - accuracy: 0.66667\n",
      "Iteration: 5850 - cost: 0.62372 - accuracy: 0.66667\n",
      "Iteration: 5900 - cost: 0.62330 - accuracy: 0.66667\n",
      "Iteration: 5950 - cost: 0.62286 - accuracy: 0.66667\n",
      "Iteration: 6000 - cost: 0.62241 - accuracy: 0.66667\n",
      "Iteration: 6050 - cost: 0.62193 - accuracy: 0.66667\n",
      "Iteration: 6100 - cost: 0.62144 - accuracy: 0.66667\n",
      "Iteration: 6150 - cost: 0.62092 - accuracy: 0.66667\n",
      "Iteration: 6200 - cost: 0.62039 - accuracy: 0.66667\n",
      "Iteration: 6250 - cost: 0.61983 - accuracy: 0.66667\n",
      "Iteration: 6300 - cost: 0.61924 - accuracy: 0.66667\n",
      "Iteration: 6350 - cost: 0.61863 - accuracy: 0.66667\n",
      "Iteration: 6400 - cost: 0.61800 - accuracy: 0.66667\n",
      "Iteration: 6450 - cost: 0.61733 - accuracy: 0.66667\n",
      "Iteration: 6500 - cost: 0.61664 - accuracy: 0.66667\n",
      "Iteration: 6550 - cost: 0.61591 - accuracy: 0.66667\n",
      "Iteration: 6600 - cost: 0.61516 - accuracy: 0.66667\n",
      "Iteration: 6650 - cost: 0.61436 - accuracy: 0.66667\n",
      "Iteration: 6700 - cost: 0.61353 - accuracy: 0.66667\n",
      "Iteration: 6750 - cost: 0.61266 - accuracy: 0.66667\n",
      "Iteration: 6800 - cost: 0.61175 - accuracy: 0.66667\n",
      "Iteration: 6850 - cost: 0.61080 - accuracy: 0.66667\n",
      "Iteration: 6900 - cost: 0.60980 - accuracy: 0.66667\n",
      "Iteration: 6950 - cost: 0.60875 - accuracy: 0.66667\n",
      "Iteration: 7000 - cost: 0.60765 - accuracy: 0.66667\n",
      "Iteration: 7050 - cost: 0.60650 - accuracy: 0.66667\n",
      "Iteration: 7100 - cost: 0.60529 - accuracy: 0.66667\n",
      "Iteration: 7150 - cost: 0.60402 - accuracy: 0.66667\n",
      "Iteration: 7200 - cost: 0.60268 - accuracy: 0.66667\n",
      "Iteration: 7250 - cost: 0.60128 - accuracy: 0.66667\n",
      "Iteration: 7300 - cost: 0.59980 - accuracy: 0.66667\n",
      "Iteration: 7350 - cost: 0.59825 - accuracy: 0.66667\n",
      "Iteration: 7400 - cost: 0.59662 - accuracy: 0.66667\n",
      "Iteration: 7450 - cost: 0.59490 - accuracy: 0.66667\n",
      "Iteration: 7500 - cost: 0.59309 - accuracy: 0.66667\n",
      "Iteration: 7550 - cost: 0.59118 - accuracy: 0.66667\n",
      "Iteration: 7600 - cost: 0.58917 - accuracy: 0.66667\n",
      "Iteration: 7650 - cost: 0.58706 - accuracy: 0.66667\n",
      "Iteration: 7700 - cost: 0.58483 - accuracy: 0.66667\n",
      "Iteration: 7750 - cost: 0.58248 - accuracy: 0.66667\n",
      "Iteration: 7800 - cost: 0.58000 - accuracy: 0.66667\n",
      "Iteration: 7850 - cost: 0.57739 - accuracy: 0.66667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7900 - cost: 0.57463 - accuracy: 0.66667\n",
      "Iteration: 7950 - cost: 0.57172 - accuracy: 0.66667\n",
      "Iteration: 8000 - cost: 0.56865 - accuracy: 0.66667\n",
      "Iteration: 8050 - cost: 0.56542 - accuracy: 0.66667\n",
      "Iteration: 8100 - cost: 0.56200 - accuracy: 0.66667\n",
      "Iteration: 8150 - cost: 0.55840 - accuracy: 0.66667\n",
      "Iteration: 8200 - cost: 0.55460 - accuracy: 0.66667\n",
      "Iteration: 8250 - cost: 0.55059 - accuracy: 0.66667\n",
      "Iteration: 8300 - cost: 0.54637 - accuracy: 0.66667\n",
      "Iteration: 8350 - cost: 0.54192 - accuracy: 0.66667\n",
      "Iteration: 8400 - cost: 0.53723 - accuracy: 0.66667\n",
      "Iteration: 8450 - cost: 0.53230 - accuracy: 0.66667\n",
      "Iteration: 8500 - cost: 0.52710 - accuracy: 0.66667\n",
      "Iteration: 8550 - cost: 0.52165 - accuracy: 0.66667\n",
      "Iteration: 8600 - cost: 0.51591 - accuracy: 0.66667\n",
      "Iteration: 8650 - cost: 0.50989 - accuracy: 0.66667\n",
      "Iteration: 8700 - cost: 0.50358 - accuracy: 0.66667\n",
      "Iteration: 8750 - cost: 0.49698 - accuracy: 0.66667\n",
      "Iteration: 8800 - cost: 0.49007 - accuracy: 0.66667\n",
      "Iteration: 8850 - cost: 0.48286 - accuracy: 0.66667\n",
      "Iteration: 8900 - cost: 0.47535 - accuracy: 0.66667\n",
      "Iteration: 8950 - cost: 0.46753 - accuracy: 0.66667\n",
      "Iteration: 9000 - cost: 0.45942 - accuracy: 0.66667\n",
      "Iteration: 9050 - cost: 0.45101 - accuracy: 0.66667\n",
      "Iteration: 9100 - cost: 0.44232 - accuracy: 0.66667\n",
      "Iteration: 9150 - cost: 0.43335 - accuracy: 0.66667\n",
      "Iteration: 9200 - cost: 0.42414 - accuracy: 0.66667\n",
      "Iteration: 9250 - cost: 0.41468 - accuracy: 0.66667\n",
      "Iteration: 9300 - cost: 0.40501 - accuracy: 0.66667\n",
      "Iteration: 9350 - cost: 0.39516 - accuracy: 0.66667\n",
      "Iteration: 9400 - cost: 0.38513 - accuracy: 0.66667\n",
      "Iteration: 9450 - cost: 0.37498 - accuracy: 0.66667\n",
      "Iteration: 9500 - cost: 0.36473 - accuracy: 0.66667\n",
      "Iteration: 9550 - cost: 0.35440 - accuracy: 0.66667\n",
      "Iteration: 9600 - cost: 0.34405 - accuracy: 0.66667\n",
      "Iteration: 9650 - cost: 0.33369 - accuracy: 0.66667\n",
      "Iteration: 9700 - cost: 0.32337 - accuracy: 0.66667\n",
      "Iteration: 9750 - cost: 0.31312 - accuracy: 0.66667\n",
      "Iteration: 9800 - cost: 0.30298 - accuracy: 0.66667\n",
      "Iteration: 9850 - cost: 0.29296 - accuracy: 0.68333\n",
      "Iteration: 9900 - cost: 0.28311 - accuracy: 0.68333\n",
      "Iteration: 9950 - cost: 0.27344 - accuracy: 0.70000\n",
      "Iteration: 10000 - cost: 0.26398 - accuracy: 0.75000\n",
      "Iteration: 10050 - cost: 0.25475 - accuracy: 0.78333\n",
      "Iteration: 10100 - cost: 0.24577 - accuracy: 0.87500\n",
      "Iteration: 10150 - cost: 0.23704 - accuracy: 0.92500\n",
      "Iteration: 10200 - cost: 0.22859 - accuracy: 0.94167\n",
      "Iteration: 10250 - cost: 0.22042 - accuracy: 0.95833\n",
      "Iteration: 10300 - cost: 0.21252 - accuracy: 0.96667\n",
      "Iteration: 10350 - cost: 0.20492 - accuracy: 0.99167\n",
      "Iteration: 10400 - cost: 0.19760 - accuracy: 1.00000\n",
      "Iteration: 10450 - cost: 0.19057 - accuracy: 1.00000\n",
      "Iteration: 10500 - cost: 0.18382 - accuracy: 1.00000\n",
      "Iteration: 10550 - cost: 0.17734 - accuracy: 1.00000\n",
      "Iteration: 10600 - cost: 0.17114 - accuracy: 1.00000\n",
      "Iteration: 10650 - cost: 0.16520 - accuracy: 1.00000\n",
      "Iteration: 10700 - cost: 0.15952 - accuracy: 1.00000\n",
      "Iteration: 10750 - cost: 0.15408 - accuracy: 1.00000\n",
      "Iteration: 10800 - cost: 0.14889 - accuracy: 1.00000\n",
      "Iteration: 10850 - cost: 0.14393 - accuracy: 1.00000\n",
      "Iteration: 10900 - cost: 0.13918 - accuracy: 1.00000\n",
      "Iteration: 10950 - cost: 0.13465 - accuracy: 1.00000\n",
      "Iteration: 11000 - cost: 0.13032 - accuracy: 1.00000\n",
      "Iteration: 11050 - cost: 0.12619 - accuracy: 1.00000\n",
      "Iteration: 11100 - cost: 0.12224 - accuracy: 1.00000\n",
      "Iteration: 11150 - cost: 0.11846 - accuracy: 1.00000\n",
      "Iteration: 11200 - cost: 0.11486 - accuracy: 1.00000\n",
      "Iteration: 11250 - cost: 0.11141 - accuracy: 1.00000\n",
      "Iteration: 11300 - cost: 0.10812 - accuracy: 1.00000\n",
      "Iteration: 11350 - cost: 0.10496 - accuracy: 1.00000\n",
      "Iteration: 11400 - cost: 0.10195 - accuracy: 1.00000\n",
      "Iteration: 11450 - cost: 0.09906 - accuracy: 1.00000\n",
      "Iteration: 11500 - cost: 0.09630 - accuracy: 1.00000\n",
      "Iteration: 11550 - cost: 0.09366 - accuracy: 1.00000\n",
      "Iteration: 11600 - cost: 0.09113 - accuracy: 1.00000\n",
      "Iteration: 11650 - cost: 0.08870 - accuracy: 1.00000\n",
      "Iteration: 11700 - cost: 0.08637 - accuracy: 1.00000\n",
      "Iteration: 11750 - cost: 0.08414 - accuracy: 1.00000\n",
      "Iteration: 11800 - cost: 0.08200 - accuracy: 1.00000\n",
      "Iteration: 11850 - cost: 0.07994 - accuracy: 1.00000\n",
      "Iteration: 11900 - cost: 0.07796 - accuracy: 1.00000\n",
      "Iteration: 11950 - cost: 0.07607 - accuracy: 1.00000\n",
      "Iteration: 12000 - cost: 0.07424 - accuracy: 1.00000\n",
      "Iteration: 12050 - cost: 0.07249 - accuracy: 1.00000\n",
      "Iteration: 12100 - cost: 0.07080 - accuracy: 1.00000\n",
      "Iteration: 12150 - cost: 0.06917 - accuracy: 1.00000\n",
      "Iteration: 12200 - cost: 0.06761 - accuracy: 1.00000\n",
      "Iteration: 12250 - cost: 0.06610 - accuracy: 1.00000\n",
      "Iteration: 12300 - cost: 0.06465 - accuracy: 1.00000\n",
      "Iteration: 12350 - cost: 0.06325 - accuracy: 1.00000\n",
      "Iteration: 12400 - cost: 0.06190 - accuracy: 1.00000\n",
      "Iteration: 12450 - cost: 0.06059 - accuracy: 1.00000\n",
      "Iteration: 12500 - cost: 0.05933 - accuracy: 1.00000\n",
      "Iteration: 12550 - cost: 0.05812 - accuracy: 1.00000\n",
      "Iteration: 12600 - cost: 0.05694 - accuracy: 1.00000\n",
      "Iteration: 12650 - cost: 0.05581 - accuracy: 1.00000\n",
      "Iteration: 12700 - cost: 0.05471 - accuracy: 1.00000\n",
      "Iteration: 12750 - cost: 0.05364 - accuracy: 1.00000\n",
      "Iteration: 12800 - cost: 0.05262 - accuracy: 1.00000\n",
      "Iteration: 12850 - cost: 0.05162 - accuracy: 1.00000\n",
      "Iteration: 12900 - cost: 0.05066 - accuracy: 1.00000\n",
      "Iteration: 12950 - cost: 0.04972 - accuracy: 1.00000\n",
      "Iteration: 13000 - cost: 0.04882 - accuracy: 1.00000\n",
      "Iteration: 13050 - cost: 0.04794 - accuracy: 1.00000\n",
      "Iteration: 13100 - cost: 0.04709 - accuracy: 1.00000\n",
      "Iteration: 13150 - cost: 0.04626 - accuracy: 1.00000\n",
      "Iteration: 13200 - cost: 0.04546 - accuracy: 1.00000\n",
      "Iteration: 13250 - cost: 0.04468 - accuracy: 1.00000\n",
      "Iteration: 13300 - cost: 0.04392 - accuracy: 1.00000\n",
      "Iteration: 13350 - cost: 0.04319 - accuracy: 1.00000\n",
      "Iteration: 13400 - cost: 0.04248 - accuracy: 1.00000\n",
      "Iteration: 13450 - cost: 0.04178 - accuracy: 1.00000\n",
      "Iteration: 13500 - cost: 0.04111 - accuracy: 1.00000\n",
      "Iteration: 13550 - cost: 0.04045 - accuracy: 1.00000\n",
      "Iteration: 13600 - cost: 0.03981 - accuracy: 1.00000\n",
      "Iteration: 13650 - cost: 0.03919 - accuracy: 1.00000\n",
      "Iteration: 13700 - cost: 0.03859 - accuracy: 1.00000\n",
      "Iteration: 13750 - cost: 0.03800 - accuracy: 1.00000\n",
      "Iteration: 13800 - cost: 0.03742 - accuracy: 1.00000\n",
      "Iteration: 13850 - cost: 0.03687 - accuracy: 1.00000\n",
      "Iteration: 13900 - cost: 0.03632 - accuracy: 1.00000\n",
      "Iteration: 13950 - cost: 0.03579 - accuracy: 1.00000\n",
      "Iteration: 14000 - cost: 0.03527 - accuracy: 1.00000\n",
      "Iteration: 14050 - cost: 0.03477 - accuracy: 1.00000\n",
      "Iteration: 14100 - cost: 0.03427 - accuracy: 1.00000\n",
      "Iteration: 14150 - cost: 0.03379 - accuracy: 1.00000\n",
      "Iteration: 14200 - cost: 0.03332 - accuracy: 1.00000\n",
      "Iteration: 14250 - cost: 0.03286 - accuracy: 1.00000\n",
      "Iteration: 14300 - cost: 0.03242 - accuracy: 1.00000\n",
      "Iteration: 14350 - cost: 0.03198 - accuracy: 1.00000\n",
      "Iteration: 14400 - cost: 0.03155 - accuracy: 1.00000\n",
      "Iteration: 14450 - cost: 0.03114 - accuracy: 1.00000\n",
      "Iteration: 14500 - cost: 0.03073 - accuracy: 1.00000\n",
      "Iteration: 14550 - cost: 0.03033 - accuracy: 1.00000\n",
      "Iteration: 14600 - cost: 0.02994 - accuracy: 1.00000\n",
      "Iteration: 14650 - cost: 0.02956 - accuracy: 1.00000\n",
      "Iteration: 14700 - cost: 0.02919 - accuracy: 1.00000\n",
      "Iteration: 14750 - cost: 0.02882 - accuracy: 1.00000\n",
      "Iteration: 14800 - cost: 0.02846 - accuracy: 1.00000\n",
      "Iteration: 14850 - cost: 0.02811 - accuracy: 1.00000\n",
      "Iteration: 14900 - cost: 0.02777 - accuracy: 1.00000\n",
      "Iteration: 14950 - cost: 0.02744 - accuracy: 1.00000\n",
      "Iteration: 15000 - cost: 0.02711 - accuracy: 1.00000\n",
      "Iteration: 15050 - cost: 0.02679 - accuracy: 1.00000\n",
      "Iteration: 15100 - cost: 0.02647 - accuracy: 1.00000\n",
      "Iteration: 15150 - cost: 0.02617 - accuracy: 1.00000\n",
      "Iteration: 15200 - cost: 0.02586 - accuracy: 1.00000\n",
      "Iteration: 15250 - cost: 0.02557 - accuracy: 1.00000\n",
      "Iteration: 15300 - cost: 0.02528 - accuracy: 1.00000\n",
      "Iteration: 15350 - cost: 0.02499 - accuracy: 1.00000\n",
      "Iteration: 15400 - cost: 0.02471 - accuracy: 1.00000\n",
      "Iteration: 15450 - cost: 0.02444 - accuracy: 1.00000\n",
      "Iteration: 15500 - cost: 0.02417 - accuracy: 1.00000\n",
      "Iteration: 15550 - cost: 0.02391 - accuracy: 1.00000\n",
      "Iteration: 15600 - cost: 0.02365 - accuracy: 1.00000\n",
      "Iteration: 15650 - cost: 0.02340 - accuracy: 1.00000\n",
      "Iteration: 15700 - cost: 0.02315 - accuracy: 1.00000\n",
      "Iteration: 15750 - cost: 0.02290 - accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15800 - cost: 0.02266 - accuracy: 1.00000\n",
      "Iteration: 15850 - cost: 0.02243 - accuracy: 1.00000\n",
      "Iteration: 15900 - cost: 0.02220 - accuracy: 1.00000\n",
      "Iteration: 15950 - cost: 0.02197 - accuracy: 1.00000\n",
      "Iteration: 16000 - cost: 0.02175 - accuracy: 1.00000\n",
      "Iteration: 16050 - cost: 0.02153 - accuracy: 1.00000\n",
      "Iteration: 16100 - cost: 0.02131 - accuracy: 1.00000\n",
      "Iteration: 16150 - cost: 0.02110 - accuracy: 1.00000\n",
      "Iteration: 16200 - cost: 0.02089 - accuracy: 1.00000\n",
      "Iteration: 16250 - cost: 0.02069 - accuracy: 1.00000\n",
      "Iteration: 16300 - cost: 0.02049 - accuracy: 1.00000\n",
      "Iteration: 16350 - cost: 0.02029 - accuracy: 1.00000\n",
      "Iteration: 16400 - cost: 0.02010 - accuracy: 1.00000\n",
      "Iteration: 16450 - cost: 0.01991 - accuracy: 1.00000\n",
      "Iteration: 16500 - cost: 0.01972 - accuracy: 1.00000\n",
      "Iteration: 16550 - cost: 0.01953 - accuracy: 1.00000\n",
      "Iteration: 16600 - cost: 0.01935 - accuracy: 1.00000\n",
      "Iteration: 16650 - cost: 0.01917 - accuracy: 1.00000\n",
      "Iteration: 16700 - cost: 0.01900 - accuracy: 1.00000\n",
      "Iteration: 16750 - cost: 0.01882 - accuracy: 1.00000\n",
      "Iteration: 16800 - cost: 0.01865 - accuracy: 1.00000\n",
      "Iteration: 16850 - cost: 0.01848 - accuracy: 1.00000\n",
      "Iteration: 16900 - cost: 0.01832 - accuracy: 1.00000\n",
      "Iteration: 16950 - cost: 0.01816 - accuracy: 1.00000\n",
      "Iteration: 17000 - cost: 0.01800 - accuracy: 1.00000\n",
      "Iteration: 17050 - cost: 0.01784 - accuracy: 1.00000\n",
      "Iteration: 17100 - cost: 0.01768 - accuracy: 1.00000\n",
      "Iteration: 17150 - cost: 0.01753 - accuracy: 1.00000\n",
      "Iteration: 17200 - cost: 0.01738 - accuracy: 1.00000\n",
      "Iteration: 17250 - cost: 0.01723 - accuracy: 1.00000\n",
      "Iteration: 17300 - cost: 0.01709 - accuracy: 1.00000\n",
      "Iteration: 17350 - cost: 0.01694 - accuracy: 1.00000\n",
      "Iteration: 17400 - cost: 0.01680 - accuracy: 1.00000\n",
      "Iteration: 17450 - cost: 0.01666 - accuracy: 1.00000\n",
      "Iteration: 17500 - cost: 0.01652 - accuracy: 1.00000\n",
      "Iteration: 17550 - cost: 0.01639 - accuracy: 1.00000\n",
      "Iteration: 17600 - cost: 0.01625 - accuracy: 1.00000\n",
      "Iteration: 17650 - cost: 0.01612 - accuracy: 1.00000\n",
      "Iteration: 17700 - cost: 0.01599 - accuracy: 1.00000\n",
      "Iteration: 17750 - cost: 0.01586 - accuracy: 1.00000\n",
      "Iteration: 17800 - cost: 0.01574 - accuracy: 1.00000\n",
      "Iteration: 17850 - cost: 0.01561 - accuracy: 1.00000\n",
      "Iteration: 17900 - cost: 0.01549 - accuracy: 1.00000\n",
      "Iteration: 17950 - cost: 0.01537 - accuracy: 1.00000\n",
      "Iteration: 18000 - cost: 0.01525 - accuracy: 1.00000\n",
      "Iteration: 18050 - cost: 0.01513 - accuracy: 1.00000\n",
      "Iteration: 18100 - cost: 0.01501 - accuracy: 1.00000\n",
      "Iteration: 18150 - cost: 0.01490 - accuracy: 1.00000\n",
      "Iteration: 18200 - cost: 0.01479 - accuracy: 1.00000\n",
      "Iteration: 18250 - cost: 0.01467 - accuracy: 1.00000\n",
      "Iteration: 18300 - cost: 0.01456 - accuracy: 1.00000\n",
      "Iteration: 18350 - cost: 0.01446 - accuracy: 1.00000\n",
      "Iteration: 18400 - cost: 0.01435 - accuracy: 1.00000\n",
      "Iteration: 18450 - cost: 0.01424 - accuracy: 1.00000\n",
      "Iteration: 18500 - cost: 0.01414 - accuracy: 1.00000\n",
      "Iteration: 18550 - cost: 0.01403 - accuracy: 1.00000\n",
      "Iteration: 18600 - cost: 0.01393 - accuracy: 1.00000\n",
      "Iteration: 18650 - cost: 0.01383 - accuracy: 1.00000\n",
      "Iteration: 18700 - cost: 0.01373 - accuracy: 1.00000\n",
      "Iteration: 18750 - cost: 0.01363 - accuracy: 1.00000\n",
      "Iteration: 18800 - cost: 0.01354 - accuracy: 1.00000\n",
      "Iteration: 18850 - cost: 0.01344 - accuracy: 1.00000\n",
      "Iteration: 18900 - cost: 0.01335 - accuracy: 1.00000\n",
      "Iteration: 18950 - cost: 0.01325 - accuracy: 1.00000\n",
      "Iteration: 19000 - cost: 0.01316 - accuracy: 1.00000\n",
      "Iteration: 19050 - cost: 0.01307 - accuracy: 1.00000\n",
      "Iteration: 19100 - cost: 0.01298 - accuracy: 1.00000\n",
      "Iteration: 19150 - cost: 0.01289 - accuracy: 1.00000\n",
      "Iteration: 19200 - cost: 0.01280 - accuracy: 1.00000\n",
      "Iteration: 19250 - cost: 0.01272 - accuracy: 1.00000\n",
      "Iteration: 19300 - cost: 0.01263 - accuracy: 1.00000\n",
      "Iteration: 19350 - cost: 0.01255 - accuracy: 1.00000\n",
      "Iteration: 19400 - cost: 0.01246 - accuracy: 1.00000\n",
      "Iteration: 19450 - cost: 0.01238 - accuracy: 1.00000\n",
      "Iteration: 19500 - cost: 0.01230 - accuracy: 1.00000\n",
      "Iteration: 19550 - cost: 0.01222 - accuracy: 1.00000\n",
      "Iteration: 19600 - cost: 0.01214 - accuracy: 1.00000\n",
      "Iteration: 19650 - cost: 0.01206 - accuracy: 1.00000\n",
      "Iteration: 19700 - cost: 0.01198 - accuracy: 1.00000\n",
      "Iteration: 19750 - cost: 0.01190 - accuracy: 1.00000\n",
      "Iteration: 19800 - cost: 0.01183 - accuracy: 1.00000\n",
      "Iteration: 19850 - cost: 0.01175 - accuracy: 1.00000\n",
      "Iteration: 19900 - cost: 0.01168 - accuracy: 1.00000\n",
      "Iteration: 19950 - cost: 0.01160 - accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "params_values, cost_history = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 20000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat, _ = forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = np.arange(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16ab6e0710>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_epochs, cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
