{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random initialization of weights for the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture):\n",
    "    # random seed initiation\n",
    "    np.random.seed(42)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for id_x, layer in enumerate(nn_architecture):\n",
    "        # count network layers from 1\n",
    "        layer_id = id_x + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_id)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_id)] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1. - sig) # calculating derivative of sigmoid function\n",
    "\n",
    "# def softmax(Z):\n",
    "#     e_z = np.exp(Z - np.max(Z))\n",
    "#     return e_z / e_z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        \n",
    "        # calculation of activation for the current layer\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "        A_curr = eval('{}(Z_curr)'.format(activ_function_curr))\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "        \n",
    "    # return predicted & saved values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        m_previous = A_prev.shape[1]\n",
    "\n",
    "        # calculation of the activation function derivative\n",
    "        dZ_curr = sigmoid_backward(dA_curr, Z_curr)\n",
    "\n",
    "        # derivative of the weights matrix \n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / m_previous \n",
    "        # derivative of the bias vector\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m_previous\n",
    "        # derivative of the previous layer activated matrix\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ < 0.3] = 0\n",
    "    probs_[probs_ >= 0.3] = 1\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        Y_hat, cashe = forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # calculating gradient\n",
    "        grads_values = backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        # updating model state\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if (i % 50 == 0):\n",
    "            print(\"Iteration: {} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            \n",
    "    return params_values, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 20, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 20, \"output_dim\": 20, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 20, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "X = (X - X.mean()) / X.std()\n",
    "y = (data.target != 0) * 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - cost: 0.78157 - accuracy: 0.66667\n",
      "Iteration: 50 - cost: 0.66789 - accuracy: 0.66667\n",
      "Iteration: 100 - cost: 0.64373 - accuracy: 0.66667\n",
      "Iteration: 150 - cost: 0.63829 - accuracy: 0.66667\n",
      "Iteration: 200 - cost: 0.63699 - accuracy: 0.66667\n",
      "Iteration: 250 - cost: 0.63665 - accuracy: 0.66667\n",
      "Iteration: 300 - cost: 0.63654 - accuracy: 0.66667\n",
      "Iteration: 350 - cost: 0.63648 - accuracy: 0.66667\n",
      "Iteration: 400 - cost: 0.63644 - accuracy: 0.66667\n",
      "Iteration: 450 - cost: 0.63640 - accuracy: 0.66667\n",
      "Iteration: 500 - cost: 0.63637 - accuracy: 0.66667\n",
      "Iteration: 550 - cost: 0.63633 - accuracy: 0.66667\n",
      "Iteration: 600 - cost: 0.63630 - accuracy: 0.66667\n",
      "Iteration: 650 - cost: 0.63626 - accuracy: 0.66667\n",
      "Iteration: 700 - cost: 0.63622 - accuracy: 0.66667\n",
      "Iteration: 750 - cost: 0.63619 - accuracy: 0.66667\n",
      "Iteration: 800 - cost: 0.63615 - accuracy: 0.66667\n",
      "Iteration: 850 - cost: 0.63611 - accuracy: 0.66667\n",
      "Iteration: 900 - cost: 0.63608 - accuracy: 0.66667\n",
      "Iteration: 950 - cost: 0.63604 - accuracy: 0.66667\n",
      "Iteration: 1000 - cost: 0.63600 - accuracy: 0.66667\n",
      "Iteration: 1050 - cost: 0.63596 - accuracy: 0.66667\n",
      "Iteration: 1100 - cost: 0.63592 - accuracy: 0.66667\n",
      "Iteration: 1150 - cost: 0.63589 - accuracy: 0.66667\n",
      "Iteration: 1200 - cost: 0.63585 - accuracy: 0.66667\n",
      "Iteration: 1250 - cost: 0.63581 - accuracy: 0.66667\n",
      "Iteration: 1300 - cost: 0.63577 - accuracy: 0.66667\n",
      "Iteration: 1350 - cost: 0.63573 - accuracy: 0.66667\n",
      "Iteration: 1400 - cost: 0.63569 - accuracy: 0.66667\n",
      "Iteration: 1450 - cost: 0.63565 - accuracy: 0.66667\n",
      "Iteration: 1500 - cost: 0.63561 - accuracy: 0.66667\n",
      "Iteration: 1550 - cost: 0.63556 - accuracy: 0.66667\n",
      "Iteration: 1600 - cost: 0.63552 - accuracy: 0.66667\n",
      "Iteration: 1650 - cost: 0.63548 - accuracy: 0.66667\n",
      "Iteration: 1700 - cost: 0.63544 - accuracy: 0.66667\n",
      "Iteration: 1750 - cost: 0.63539 - accuracy: 0.66667\n",
      "Iteration: 1800 - cost: 0.63535 - accuracy: 0.66667\n",
      "Iteration: 1850 - cost: 0.63530 - accuracy: 0.66667\n",
      "Iteration: 1900 - cost: 0.63525 - accuracy: 0.66667\n",
      "Iteration: 1950 - cost: 0.63521 - accuracy: 0.66667\n",
      "Iteration: 2000 - cost: 0.63516 - accuracy: 0.66667\n",
      "Iteration: 2050 - cost: 0.63511 - accuracy: 0.66667\n",
      "Iteration: 2100 - cost: 0.63506 - accuracy: 0.66667\n",
      "Iteration: 2150 - cost: 0.63501 - accuracy: 0.66667\n",
      "Iteration: 2200 - cost: 0.63496 - accuracy: 0.66667\n",
      "Iteration: 2250 - cost: 0.63491 - accuracy: 0.66667\n",
      "Iteration: 2300 - cost: 0.63486 - accuracy: 0.66667\n",
      "Iteration: 2350 - cost: 0.63480 - accuracy: 0.66667\n",
      "Iteration: 2400 - cost: 0.63475 - accuracy: 0.66667\n",
      "Iteration: 2450 - cost: 0.63469 - accuracy: 0.66667\n",
      "Iteration: 2500 - cost: 0.63464 - accuracy: 0.66667\n",
      "Iteration: 2550 - cost: 0.63458 - accuracy: 0.66667\n",
      "Iteration: 2600 - cost: 0.63452 - accuracy: 0.66667\n",
      "Iteration: 2650 - cost: 0.63446 - accuracy: 0.66667\n",
      "Iteration: 2700 - cost: 0.63440 - accuracy: 0.66667\n",
      "Iteration: 2750 - cost: 0.63434 - accuracy: 0.66667\n",
      "Iteration: 2800 - cost: 0.63427 - accuracy: 0.66667\n",
      "Iteration: 2850 - cost: 0.63421 - accuracy: 0.66667\n",
      "Iteration: 2900 - cost: 0.63414 - accuracy: 0.66667\n",
      "Iteration: 2950 - cost: 0.63407 - accuracy: 0.66667\n",
      "Iteration: 3000 - cost: 0.63400 - accuracy: 0.66667\n",
      "Iteration: 3050 - cost: 0.63393 - accuracy: 0.66667\n",
      "Iteration: 3100 - cost: 0.63385 - accuracy: 0.66667\n",
      "Iteration: 3150 - cost: 0.63378 - accuracy: 0.66667\n",
      "Iteration: 3200 - cost: 0.63370 - accuracy: 0.66667\n",
      "Iteration: 3250 - cost: 0.63362 - accuracy: 0.66667\n",
      "Iteration: 3300 - cost: 0.63354 - accuracy: 0.66667\n",
      "Iteration: 3350 - cost: 0.63346 - accuracy: 0.66667\n",
      "Iteration: 3400 - cost: 0.63337 - accuracy: 0.66667\n",
      "Iteration: 3450 - cost: 0.63329 - accuracy: 0.66667\n",
      "Iteration: 3500 - cost: 0.63320 - accuracy: 0.66667\n",
      "Iteration: 3550 - cost: 0.63311 - accuracy: 0.66667\n",
      "Iteration: 3600 - cost: 0.63301 - accuracy: 0.66667\n",
      "Iteration: 3650 - cost: 0.63292 - accuracy: 0.66667\n",
      "Iteration: 3700 - cost: 0.63282 - accuracy: 0.66667\n",
      "Iteration: 3750 - cost: 0.63271 - accuracy: 0.66667\n",
      "Iteration: 3800 - cost: 0.63261 - accuracy: 0.66667\n",
      "Iteration: 3850 - cost: 0.63250 - accuracy: 0.66667\n",
      "Iteration: 3900 - cost: 0.63239 - accuracy: 0.66667\n",
      "Iteration: 3950 - cost: 0.63228 - accuracy: 0.66667\n",
      "Iteration: 4000 - cost: 0.63216 - accuracy: 0.66667\n",
      "Iteration: 4050 - cost: 0.63204 - accuracy: 0.66667\n",
      "Iteration: 4100 - cost: 0.63192 - accuracy: 0.66667\n",
      "Iteration: 4150 - cost: 0.63179 - accuracy: 0.66667\n",
      "Iteration: 4200 - cost: 0.63166 - accuracy: 0.66667\n",
      "Iteration: 4250 - cost: 0.63152 - accuracy: 0.66667\n",
      "Iteration: 4300 - cost: 0.63138 - accuracy: 0.66667\n",
      "Iteration: 4350 - cost: 0.63124 - accuracy: 0.66667\n",
      "Iteration: 4400 - cost: 0.63109 - accuracy: 0.66667\n",
      "Iteration: 4450 - cost: 0.63094 - accuracy: 0.66667\n",
      "Iteration: 4500 - cost: 0.63078 - accuracy: 0.66667\n",
      "Iteration: 4550 - cost: 0.63062 - accuracy: 0.66667\n",
      "Iteration: 4600 - cost: 0.63045 - accuracy: 0.66667\n",
      "Iteration: 4650 - cost: 0.63028 - accuracy: 0.66667\n",
      "Iteration: 4700 - cost: 0.63010 - accuracy: 0.66667\n",
      "Iteration: 4750 - cost: 0.62992 - accuracy: 0.66667\n",
      "Iteration: 4800 - cost: 0.62973 - accuracy: 0.66667\n",
      "Iteration: 4850 - cost: 0.62953 - accuracy: 0.66667\n",
      "Iteration: 4900 - cost: 0.62933 - accuracy: 0.66667\n",
      "Iteration: 4950 - cost: 0.62912 - accuracy: 0.66667\n",
      "Iteration: 5000 - cost: 0.62890 - accuracy: 0.66667\n",
      "Iteration: 5050 - cost: 0.62867 - accuracy: 0.66667\n",
      "Iteration: 5100 - cost: 0.62844 - accuracy: 0.66667\n",
      "Iteration: 5150 - cost: 0.62820 - accuracy: 0.66667\n",
      "Iteration: 5200 - cost: 0.62795 - accuracy: 0.66667\n",
      "Iteration: 5250 - cost: 0.62769 - accuracy: 0.66667\n",
      "Iteration: 5300 - cost: 0.62742 - accuracy: 0.66667\n",
      "Iteration: 5350 - cost: 0.62714 - accuracy: 0.66667\n",
      "Iteration: 5400 - cost: 0.62686 - accuracy: 0.66667\n",
      "Iteration: 5450 - cost: 0.62656 - accuracy: 0.66667\n",
      "Iteration: 5500 - cost: 0.62625 - accuracy: 0.66667\n",
      "Iteration: 5550 - cost: 0.62593 - accuracy: 0.66667\n",
      "Iteration: 5600 - cost: 0.62559 - accuracy: 0.66667\n",
      "Iteration: 5650 - cost: 0.62525 - accuracy: 0.66667\n",
      "Iteration: 5700 - cost: 0.62489 - accuracy: 0.66667\n",
      "Iteration: 5750 - cost: 0.62451 - accuracy: 0.66667\n",
      "Iteration: 5800 - cost: 0.62412 - accuracy: 0.66667\n",
      "Iteration: 5850 - cost: 0.62372 - accuracy: 0.66667\n",
      "Iteration: 5900 - cost: 0.62330 - accuracy: 0.66667\n",
      "Iteration: 5950 - cost: 0.62286 - accuracy: 0.66667\n",
      "Iteration: 6000 - cost: 0.62241 - accuracy: 0.66667\n",
      "Iteration: 6050 - cost: 0.62193 - accuracy: 0.66667\n",
      "Iteration: 6100 - cost: 0.62144 - accuracy: 0.66667\n",
      "Iteration: 6150 - cost: 0.62092 - accuracy: 0.66667\n",
      "Iteration: 6200 - cost: 0.62039 - accuracy: 0.66667\n",
      "Iteration: 6250 - cost: 0.61983 - accuracy: 0.66667\n",
      "Iteration: 6300 - cost: 0.61924 - accuracy: 0.66667\n",
      "Iteration: 6350 - cost: 0.61863 - accuracy: 0.66667\n",
      "Iteration: 6400 - cost: 0.61800 - accuracy: 0.66667\n",
      "Iteration: 6450 - cost: 0.61733 - accuracy: 0.66667\n",
      "Iteration: 6500 - cost: 0.61664 - accuracy: 0.66667\n",
      "Iteration: 6550 - cost: 0.61591 - accuracy: 0.66667\n",
      "Iteration: 6600 - cost: 0.61516 - accuracy: 0.66667\n",
      "Iteration: 6650 - cost: 0.61436 - accuracy: 0.66667\n",
      "Iteration: 6700 - cost: 0.61353 - accuracy: 0.66667\n",
      "Iteration: 6750 - cost: 0.61266 - accuracy: 0.66667\n",
      "Iteration: 6800 - cost: 0.61175 - accuracy: 0.66667\n",
      "Iteration: 6850 - cost: 0.61080 - accuracy: 0.66667\n",
      "Iteration: 6900 - cost: 0.60980 - accuracy: 0.66667\n",
      "Iteration: 6950 - cost: 0.60875 - accuracy: 0.66667\n",
      "Iteration: 7000 - cost: 0.60765 - accuracy: 0.66667\n",
      "Iteration: 7050 - cost: 0.60650 - accuracy: 0.66667\n",
      "Iteration: 7100 - cost: 0.60529 - accuracy: 0.66667\n",
      "Iteration: 7150 - cost: 0.60402 - accuracy: 0.66667\n",
      "Iteration: 7200 - cost: 0.60268 - accuracy: 0.66667\n",
      "Iteration: 7250 - cost: 0.60128 - accuracy: 0.66667\n",
      "Iteration: 7300 - cost: 0.59980 - accuracy: 0.66667\n",
      "Iteration: 7350 - cost: 0.59825 - accuracy: 0.66667\n",
      "Iteration: 7400 - cost: 0.59662 - accuracy: 0.66667\n",
      "Iteration: 7450 - cost: 0.59490 - accuracy: 0.66667\n",
      "Iteration: 7500 - cost: 0.59309 - accuracy: 0.66667\n",
      "Iteration: 7550 - cost: 0.59118 - accuracy: 0.66667\n",
      "Iteration: 7600 - cost: 0.58917 - accuracy: 0.66667\n",
      "Iteration: 7650 - cost: 0.58706 - accuracy: 0.66667\n",
      "Iteration: 7700 - cost: 0.58483 - accuracy: 0.66667\n",
      "Iteration: 7750 - cost: 0.58248 - accuracy: 0.66667\n",
      "Iteration: 7800 - cost: 0.58000 - accuracy: 0.66667\n",
      "Iteration: 7850 - cost: 0.57739 - accuracy: 0.66667\n",
      "Iteration: 7900 - cost: 0.57463 - accuracy: 0.66667\n",
      "Iteration: 7950 - cost: 0.57172 - accuracy: 0.66667\n",
      "Iteration: 8000 - cost: 0.56865 - accuracy: 0.66667\n",
      "Iteration: 8050 - cost: 0.56542 - accuracy: 0.66667\n",
      "Iteration: 8100 - cost: 0.56200 - accuracy: 0.66667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8150 - cost: 0.55840 - accuracy: 0.66667\n",
      "Iteration: 8200 - cost: 0.55460 - accuracy: 0.66667\n",
      "Iteration: 8250 - cost: 0.55059 - accuracy: 0.66667\n",
      "Iteration: 8300 - cost: 0.54637 - accuracy: 0.66667\n",
      "Iteration: 8350 - cost: 0.54192 - accuracy: 0.66667\n",
      "Iteration: 8400 - cost: 0.53723 - accuracy: 0.66667\n",
      "Iteration: 8450 - cost: 0.53230 - accuracy: 0.66667\n",
      "Iteration: 8500 - cost: 0.52710 - accuracy: 0.66667\n",
      "Iteration: 8550 - cost: 0.52165 - accuracy: 0.66667\n",
      "Iteration: 8600 - cost: 0.51591 - accuracy: 0.66667\n",
      "Iteration: 8650 - cost: 0.50989 - accuracy: 0.66667\n",
      "Iteration: 8700 - cost: 0.50358 - accuracy: 0.66667\n",
      "Iteration: 8750 - cost: 0.49698 - accuracy: 0.66667\n",
      "Iteration: 8800 - cost: 0.49007 - accuracy: 0.66667\n",
      "Iteration: 8850 - cost: 0.48286 - accuracy: 0.66667\n",
      "Iteration: 8900 - cost: 0.47535 - accuracy: 0.66667\n",
      "Iteration: 8950 - cost: 0.46753 - accuracy: 0.66667\n",
      "Iteration: 9000 - cost: 0.45942 - accuracy: 0.66667\n",
      "Iteration: 9050 - cost: 0.45101 - accuracy: 0.66667\n",
      "Iteration: 9100 - cost: 0.44232 - accuracy: 0.66667\n",
      "Iteration: 9150 - cost: 0.43335 - accuracy: 0.66667\n",
      "Iteration: 9200 - cost: 0.42414 - accuracy: 0.66667\n",
      "Iteration: 9250 - cost: 0.41468 - accuracy: 0.66667\n",
      "Iteration: 9300 - cost: 0.40501 - accuracy: 0.66667\n",
      "Iteration: 9350 - cost: 0.39516 - accuracy: 0.66667\n",
      "Iteration: 9400 - cost: 0.38513 - accuracy: 0.66667\n",
      "Iteration: 9450 - cost: 0.37498 - accuracy: 0.66667\n",
      "Iteration: 9500 - cost: 0.36473 - accuracy: 0.66667\n",
      "Iteration: 9550 - cost: 0.35440 - accuracy: 0.66667\n",
      "Iteration: 9600 - cost: 0.34405 - accuracy: 0.66667\n",
      "Iteration: 9650 - cost: 0.33369 - accuracy: 0.66667\n",
      "Iteration: 9700 - cost: 0.32337 - accuracy: 0.66667\n",
      "Iteration: 9750 - cost: 0.31312 - accuracy: 0.66667\n",
      "Iteration: 9800 - cost: 0.30298 - accuracy: 0.66667\n",
      "Iteration: 9850 - cost: 0.29296 - accuracy: 0.66667\n",
      "Iteration: 9900 - cost: 0.28311 - accuracy: 0.66667\n",
      "Iteration: 9950 - cost: 0.27344 - accuracy: 0.66667\n",
      "Iteration: 10000 - cost: 0.26398 - accuracy: 0.68333\n",
      "Iteration: 10050 - cost: 0.25475 - accuracy: 0.68333\n",
      "Iteration: 10100 - cost: 0.24577 - accuracy: 0.70000\n",
      "Iteration: 10150 - cost: 0.23704 - accuracy: 0.75833\n",
      "Iteration: 10200 - cost: 0.22859 - accuracy: 0.78333\n",
      "Iteration: 10250 - cost: 0.22042 - accuracy: 0.87500\n",
      "Iteration: 10300 - cost: 0.21252 - accuracy: 0.92500\n",
      "Iteration: 10350 - cost: 0.20492 - accuracy: 0.94167\n",
      "Iteration: 10400 - cost: 0.19760 - accuracy: 0.95833\n",
      "Iteration: 10450 - cost: 0.19057 - accuracy: 0.95833\n",
      "Iteration: 10500 - cost: 0.18382 - accuracy: 0.99167\n",
      "Iteration: 10550 - cost: 0.17734 - accuracy: 0.99167\n",
      "Iteration: 10600 - cost: 0.17114 - accuracy: 1.00000\n",
      "Iteration: 10650 - cost: 0.16520 - accuracy: 1.00000\n",
      "Iteration: 10700 - cost: 0.15952 - accuracy: 1.00000\n",
      "Iteration: 10750 - cost: 0.15408 - accuracy: 1.00000\n",
      "Iteration: 10800 - cost: 0.14889 - accuracy: 1.00000\n",
      "Iteration: 10850 - cost: 0.14393 - accuracy: 1.00000\n",
      "Iteration: 10900 - cost: 0.13918 - accuracy: 1.00000\n",
      "Iteration: 10950 - cost: 0.13465 - accuracy: 1.00000\n",
      "Iteration: 11000 - cost: 0.13032 - accuracy: 1.00000\n",
      "Iteration: 11050 - cost: 0.12619 - accuracy: 1.00000\n",
      "Iteration: 11100 - cost: 0.12224 - accuracy: 1.00000\n",
      "Iteration: 11150 - cost: 0.11846 - accuracy: 1.00000\n",
      "Iteration: 11200 - cost: 0.11486 - accuracy: 1.00000\n",
      "Iteration: 11250 - cost: 0.11141 - accuracy: 1.00000\n",
      "Iteration: 11300 - cost: 0.10812 - accuracy: 1.00000\n",
      "Iteration: 11350 - cost: 0.10496 - accuracy: 1.00000\n",
      "Iteration: 11400 - cost: 0.10195 - accuracy: 1.00000\n",
      "Iteration: 11450 - cost: 0.09906 - accuracy: 1.00000\n",
      "Iteration: 11500 - cost: 0.09630 - accuracy: 1.00000\n",
      "Iteration: 11550 - cost: 0.09366 - accuracy: 1.00000\n",
      "Iteration: 11600 - cost: 0.09113 - accuracy: 1.00000\n",
      "Iteration: 11650 - cost: 0.08870 - accuracy: 1.00000\n",
      "Iteration: 11700 - cost: 0.08637 - accuracy: 1.00000\n",
      "Iteration: 11750 - cost: 0.08414 - accuracy: 1.00000\n",
      "Iteration: 11800 - cost: 0.08200 - accuracy: 1.00000\n",
      "Iteration: 11850 - cost: 0.07994 - accuracy: 1.00000\n",
      "Iteration: 11900 - cost: 0.07796 - accuracy: 1.00000\n",
      "Iteration: 11950 - cost: 0.07607 - accuracy: 1.00000\n",
      "Iteration: 12000 - cost: 0.07424 - accuracy: 1.00000\n",
      "Iteration: 12050 - cost: 0.07249 - accuracy: 1.00000\n",
      "Iteration: 12100 - cost: 0.07080 - accuracy: 1.00000\n",
      "Iteration: 12150 - cost: 0.06917 - accuracy: 1.00000\n",
      "Iteration: 12200 - cost: 0.06761 - accuracy: 1.00000\n",
      "Iteration: 12250 - cost: 0.06610 - accuracy: 1.00000\n",
      "Iteration: 12300 - cost: 0.06465 - accuracy: 1.00000\n",
      "Iteration: 12350 - cost: 0.06325 - accuracy: 1.00000\n",
      "Iteration: 12400 - cost: 0.06190 - accuracy: 1.00000\n",
      "Iteration: 12450 - cost: 0.06059 - accuracy: 1.00000\n",
      "Iteration: 12500 - cost: 0.05933 - accuracy: 1.00000\n",
      "Iteration: 12550 - cost: 0.05812 - accuracy: 1.00000\n",
      "Iteration: 12600 - cost: 0.05694 - accuracy: 1.00000\n",
      "Iteration: 12650 - cost: 0.05581 - accuracy: 1.00000\n",
      "Iteration: 12700 - cost: 0.05471 - accuracy: 1.00000\n",
      "Iteration: 12750 - cost: 0.05364 - accuracy: 1.00000\n",
      "Iteration: 12800 - cost: 0.05262 - accuracy: 1.00000\n",
      "Iteration: 12850 - cost: 0.05162 - accuracy: 1.00000\n",
      "Iteration: 12900 - cost: 0.05066 - accuracy: 1.00000\n",
      "Iteration: 12950 - cost: 0.04972 - accuracy: 1.00000\n",
      "Iteration: 13000 - cost: 0.04882 - accuracy: 1.00000\n",
      "Iteration: 13050 - cost: 0.04794 - accuracy: 1.00000\n",
      "Iteration: 13100 - cost: 0.04709 - accuracy: 1.00000\n",
      "Iteration: 13150 - cost: 0.04626 - accuracy: 1.00000\n",
      "Iteration: 13200 - cost: 0.04546 - accuracy: 1.00000\n",
      "Iteration: 13250 - cost: 0.04468 - accuracy: 1.00000\n",
      "Iteration: 13300 - cost: 0.04392 - accuracy: 1.00000\n",
      "Iteration: 13350 - cost: 0.04319 - accuracy: 1.00000\n",
      "Iteration: 13400 - cost: 0.04248 - accuracy: 1.00000\n",
      "Iteration: 13450 - cost: 0.04178 - accuracy: 1.00000\n",
      "Iteration: 13500 - cost: 0.04111 - accuracy: 1.00000\n",
      "Iteration: 13550 - cost: 0.04045 - accuracy: 1.00000\n",
      "Iteration: 13600 - cost: 0.03981 - accuracy: 1.00000\n",
      "Iteration: 13650 - cost: 0.03919 - accuracy: 1.00000\n",
      "Iteration: 13700 - cost: 0.03859 - accuracy: 1.00000\n",
      "Iteration: 13750 - cost: 0.03800 - accuracy: 1.00000\n",
      "Iteration: 13800 - cost: 0.03742 - accuracy: 1.00000\n",
      "Iteration: 13850 - cost: 0.03687 - accuracy: 1.00000\n",
      "Iteration: 13900 - cost: 0.03632 - accuracy: 1.00000\n",
      "Iteration: 13950 - cost: 0.03579 - accuracy: 1.00000\n",
      "Iteration: 14000 - cost: 0.03527 - accuracy: 1.00000\n",
      "Iteration: 14050 - cost: 0.03477 - accuracy: 1.00000\n",
      "Iteration: 14100 - cost: 0.03427 - accuracy: 1.00000\n",
      "Iteration: 14150 - cost: 0.03379 - accuracy: 1.00000\n",
      "Iteration: 14200 - cost: 0.03332 - accuracy: 1.00000\n",
      "Iteration: 14250 - cost: 0.03286 - accuracy: 1.00000\n",
      "Iteration: 14300 - cost: 0.03242 - accuracy: 1.00000\n",
      "Iteration: 14350 - cost: 0.03198 - accuracy: 1.00000\n",
      "Iteration: 14400 - cost: 0.03155 - accuracy: 1.00000\n",
      "Iteration: 14450 - cost: 0.03114 - accuracy: 1.00000\n",
      "Iteration: 14500 - cost: 0.03073 - accuracy: 1.00000\n",
      "Iteration: 14550 - cost: 0.03033 - accuracy: 1.00000\n",
      "Iteration: 14600 - cost: 0.02994 - accuracy: 1.00000\n",
      "Iteration: 14650 - cost: 0.02956 - accuracy: 1.00000\n",
      "Iteration: 14700 - cost: 0.02919 - accuracy: 1.00000\n",
      "Iteration: 14750 - cost: 0.02882 - accuracy: 1.00000\n",
      "Iteration: 14800 - cost: 0.02846 - accuracy: 1.00000\n",
      "Iteration: 14850 - cost: 0.02811 - accuracy: 1.00000\n",
      "Iteration: 14900 - cost: 0.02777 - accuracy: 1.00000\n",
      "Iteration: 14950 - cost: 0.02744 - accuracy: 1.00000\n",
      "Iteration: 15000 - cost: 0.02711 - accuracy: 1.00000\n",
      "Iteration: 15050 - cost: 0.02679 - accuracy: 1.00000\n",
      "Iteration: 15100 - cost: 0.02647 - accuracy: 1.00000\n",
      "Iteration: 15150 - cost: 0.02617 - accuracy: 1.00000\n",
      "Iteration: 15200 - cost: 0.02586 - accuracy: 1.00000\n",
      "Iteration: 15250 - cost: 0.02557 - accuracy: 1.00000\n",
      "Iteration: 15300 - cost: 0.02528 - accuracy: 1.00000\n",
      "Iteration: 15350 - cost: 0.02499 - accuracy: 1.00000\n",
      "Iteration: 15400 - cost: 0.02471 - accuracy: 1.00000\n",
      "Iteration: 15450 - cost: 0.02444 - accuracy: 1.00000\n",
      "Iteration: 15500 - cost: 0.02417 - accuracy: 1.00000\n",
      "Iteration: 15550 - cost: 0.02391 - accuracy: 1.00000\n",
      "Iteration: 15600 - cost: 0.02365 - accuracy: 1.00000\n",
      "Iteration: 15650 - cost: 0.02340 - accuracy: 1.00000\n",
      "Iteration: 15700 - cost: 0.02315 - accuracy: 1.00000\n",
      "Iteration: 15750 - cost: 0.02290 - accuracy: 1.00000\n",
      "Iteration: 15800 - cost: 0.02266 - accuracy: 1.00000\n",
      "Iteration: 15850 - cost: 0.02243 - accuracy: 1.00000\n",
      "Iteration: 15900 - cost: 0.02220 - accuracy: 1.00000\n",
      "Iteration: 15950 - cost: 0.02197 - accuracy: 1.00000\n",
      "Iteration: 16000 - cost: 0.02175 - accuracy: 1.00000\n",
      "Iteration: 16050 - cost: 0.02153 - accuracy: 1.00000\n",
      "Iteration: 16100 - cost: 0.02131 - accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 16150 - cost: 0.02110 - accuracy: 1.00000\n",
      "Iteration: 16200 - cost: 0.02089 - accuracy: 1.00000\n",
      "Iteration: 16250 - cost: 0.02069 - accuracy: 1.00000\n",
      "Iteration: 16300 - cost: 0.02049 - accuracy: 1.00000\n",
      "Iteration: 16350 - cost: 0.02029 - accuracy: 1.00000\n",
      "Iteration: 16400 - cost: 0.02010 - accuracy: 1.00000\n",
      "Iteration: 16450 - cost: 0.01991 - accuracy: 1.00000\n",
      "Iteration: 16500 - cost: 0.01972 - accuracy: 1.00000\n",
      "Iteration: 16550 - cost: 0.01953 - accuracy: 1.00000\n",
      "Iteration: 16600 - cost: 0.01935 - accuracy: 1.00000\n",
      "Iteration: 16650 - cost: 0.01917 - accuracy: 1.00000\n",
      "Iteration: 16700 - cost: 0.01900 - accuracy: 1.00000\n",
      "Iteration: 16750 - cost: 0.01882 - accuracy: 1.00000\n",
      "Iteration: 16800 - cost: 0.01865 - accuracy: 1.00000\n",
      "Iteration: 16850 - cost: 0.01848 - accuracy: 1.00000\n",
      "Iteration: 16900 - cost: 0.01832 - accuracy: 1.00000\n",
      "Iteration: 16950 - cost: 0.01816 - accuracy: 1.00000\n",
      "Iteration: 17000 - cost: 0.01800 - accuracy: 1.00000\n",
      "Iteration: 17050 - cost: 0.01784 - accuracy: 1.00000\n",
      "Iteration: 17100 - cost: 0.01768 - accuracy: 1.00000\n",
      "Iteration: 17150 - cost: 0.01753 - accuracy: 1.00000\n",
      "Iteration: 17200 - cost: 0.01738 - accuracy: 1.00000\n",
      "Iteration: 17250 - cost: 0.01723 - accuracy: 1.00000\n",
      "Iteration: 17300 - cost: 0.01709 - accuracy: 1.00000\n",
      "Iteration: 17350 - cost: 0.01694 - accuracy: 1.00000\n",
      "Iteration: 17400 - cost: 0.01680 - accuracy: 1.00000\n",
      "Iteration: 17450 - cost: 0.01666 - accuracy: 1.00000\n",
      "Iteration: 17500 - cost: 0.01652 - accuracy: 1.00000\n",
      "Iteration: 17550 - cost: 0.01639 - accuracy: 1.00000\n",
      "Iteration: 17600 - cost: 0.01625 - accuracy: 1.00000\n",
      "Iteration: 17650 - cost: 0.01612 - accuracy: 1.00000\n",
      "Iteration: 17700 - cost: 0.01599 - accuracy: 1.00000\n",
      "Iteration: 17750 - cost: 0.01586 - accuracy: 1.00000\n",
      "Iteration: 17800 - cost: 0.01574 - accuracy: 1.00000\n",
      "Iteration: 17850 - cost: 0.01561 - accuracy: 1.00000\n",
      "Iteration: 17900 - cost: 0.01549 - accuracy: 1.00000\n",
      "Iteration: 17950 - cost: 0.01537 - accuracy: 1.00000\n",
      "Iteration: 18000 - cost: 0.01525 - accuracy: 1.00000\n",
      "Iteration: 18050 - cost: 0.01513 - accuracy: 1.00000\n",
      "Iteration: 18100 - cost: 0.01501 - accuracy: 1.00000\n",
      "Iteration: 18150 - cost: 0.01490 - accuracy: 1.00000\n",
      "Iteration: 18200 - cost: 0.01479 - accuracy: 1.00000\n",
      "Iteration: 18250 - cost: 0.01467 - accuracy: 1.00000\n",
      "Iteration: 18300 - cost: 0.01456 - accuracy: 1.00000\n",
      "Iteration: 18350 - cost: 0.01446 - accuracy: 1.00000\n",
      "Iteration: 18400 - cost: 0.01435 - accuracy: 1.00000\n",
      "Iteration: 18450 - cost: 0.01424 - accuracy: 1.00000\n",
      "Iteration: 18500 - cost: 0.01414 - accuracy: 1.00000\n",
      "Iteration: 18550 - cost: 0.01403 - accuracy: 1.00000\n",
      "Iteration: 18600 - cost: 0.01393 - accuracy: 1.00000\n",
      "Iteration: 18650 - cost: 0.01383 - accuracy: 1.00000\n",
      "Iteration: 18700 - cost: 0.01373 - accuracy: 1.00000\n",
      "Iteration: 18750 - cost: 0.01363 - accuracy: 1.00000\n",
      "Iteration: 18800 - cost: 0.01354 - accuracy: 1.00000\n",
      "Iteration: 18850 - cost: 0.01344 - accuracy: 1.00000\n",
      "Iteration: 18900 - cost: 0.01335 - accuracy: 1.00000\n",
      "Iteration: 18950 - cost: 0.01325 - accuracy: 1.00000\n",
      "Iteration: 19000 - cost: 0.01316 - accuracy: 1.00000\n",
      "Iteration: 19050 - cost: 0.01307 - accuracy: 1.00000\n",
      "Iteration: 19100 - cost: 0.01298 - accuracy: 1.00000\n",
      "Iteration: 19150 - cost: 0.01289 - accuracy: 1.00000\n",
      "Iteration: 19200 - cost: 0.01280 - accuracy: 1.00000\n",
      "Iteration: 19250 - cost: 0.01272 - accuracy: 1.00000\n",
      "Iteration: 19300 - cost: 0.01263 - accuracy: 1.00000\n",
      "Iteration: 19350 - cost: 0.01255 - accuracy: 1.00000\n",
      "Iteration: 19400 - cost: 0.01246 - accuracy: 1.00000\n",
      "Iteration: 19450 - cost: 0.01238 - accuracy: 1.00000\n",
      "Iteration: 19500 - cost: 0.01230 - accuracy: 1.00000\n",
      "Iteration: 19550 - cost: 0.01222 - accuracy: 1.00000\n",
      "Iteration: 19600 - cost: 0.01214 - accuracy: 1.00000\n",
      "Iteration: 19650 - cost: 0.01206 - accuracy: 1.00000\n",
      "Iteration: 19700 - cost: 0.01198 - accuracy: 1.00000\n",
      "Iteration: 19750 - cost: 0.01190 - accuracy: 1.00000\n",
      "Iteration: 19800 - cost: 0.01183 - accuracy: 1.00000\n",
      "Iteration: 19850 - cost: 0.01175 - accuracy: 1.00000\n",
      "Iteration: 19900 - cost: 0.01168 - accuracy: 1.00000\n",
      "Iteration: 19950 - cost: 0.01160 - accuracy: 1.00000\n",
      "Iteration: 20000 - cost: 0.01153 - accuracy: 1.00000\n",
      "Iteration: 20050 - cost: 0.01146 - accuracy: 1.00000\n",
      "Iteration: 20100 - cost: 0.01139 - accuracy: 1.00000\n",
      "Iteration: 20150 - cost: 0.01132 - accuracy: 1.00000\n",
      "Iteration: 20200 - cost: 0.01125 - accuracy: 1.00000\n",
      "Iteration: 20250 - cost: 0.01118 - accuracy: 1.00000\n",
      "Iteration: 20300 - cost: 0.01111 - accuracy: 1.00000\n",
      "Iteration: 20350 - cost: 0.01104 - accuracy: 1.00000\n",
      "Iteration: 20400 - cost: 0.01097 - accuracy: 1.00000\n",
      "Iteration: 20450 - cost: 0.01091 - accuracy: 1.00000\n",
      "Iteration: 20500 - cost: 0.01084 - accuracy: 1.00000\n",
      "Iteration: 20550 - cost: 0.01078 - accuracy: 1.00000\n",
      "Iteration: 20600 - cost: 0.01071 - accuracy: 1.00000\n",
      "Iteration: 20650 - cost: 0.01065 - accuracy: 1.00000\n",
      "Iteration: 20700 - cost: 0.01059 - accuracy: 1.00000\n",
      "Iteration: 20750 - cost: 0.01053 - accuracy: 1.00000\n",
      "Iteration: 20800 - cost: 0.01046 - accuracy: 1.00000\n",
      "Iteration: 20850 - cost: 0.01040 - accuracy: 1.00000\n",
      "Iteration: 20900 - cost: 0.01034 - accuracy: 1.00000\n",
      "Iteration: 20950 - cost: 0.01028 - accuracy: 1.00000\n",
      "Iteration: 21000 - cost: 0.01022 - accuracy: 1.00000\n",
      "Iteration: 21050 - cost: 0.01017 - accuracy: 1.00000\n",
      "Iteration: 21100 - cost: 0.01011 - accuracy: 1.00000\n",
      "Iteration: 21150 - cost: 0.01005 - accuracy: 1.00000\n",
      "Iteration: 21200 - cost: 0.00999 - accuracy: 1.00000\n",
      "Iteration: 21250 - cost: 0.00994 - accuracy: 1.00000\n",
      "Iteration: 21300 - cost: 0.00988 - accuracy: 1.00000\n",
      "Iteration: 21350 - cost: 0.00983 - accuracy: 1.00000\n",
      "Iteration: 21400 - cost: 0.00977 - accuracy: 1.00000\n",
      "Iteration: 21450 - cost: 0.00972 - accuracy: 1.00000\n",
      "Iteration: 21500 - cost: 0.00967 - accuracy: 1.00000\n",
      "Iteration: 21550 - cost: 0.00961 - accuracy: 1.00000\n",
      "Iteration: 21600 - cost: 0.00956 - accuracy: 1.00000\n",
      "Iteration: 21650 - cost: 0.00951 - accuracy: 1.00000\n",
      "Iteration: 21700 - cost: 0.00946 - accuracy: 1.00000\n",
      "Iteration: 21750 - cost: 0.00941 - accuracy: 1.00000\n",
      "Iteration: 21800 - cost: 0.00936 - accuracy: 1.00000\n",
      "Iteration: 21850 - cost: 0.00931 - accuracy: 1.00000\n",
      "Iteration: 21900 - cost: 0.00926 - accuracy: 1.00000\n",
      "Iteration: 21950 - cost: 0.00921 - accuracy: 1.00000\n",
      "Iteration: 22000 - cost: 0.00916 - accuracy: 1.00000\n",
      "Iteration: 22050 - cost: 0.00911 - accuracy: 1.00000\n",
      "Iteration: 22100 - cost: 0.00906 - accuracy: 1.00000\n",
      "Iteration: 22150 - cost: 0.00902 - accuracy: 1.00000\n",
      "Iteration: 22200 - cost: 0.00897 - accuracy: 1.00000\n",
      "Iteration: 22250 - cost: 0.00892 - accuracy: 1.00000\n",
      "Iteration: 22300 - cost: 0.00888 - accuracy: 1.00000\n",
      "Iteration: 22350 - cost: 0.00883 - accuracy: 1.00000\n",
      "Iteration: 22400 - cost: 0.00879 - accuracy: 1.00000\n",
      "Iteration: 22450 - cost: 0.00874 - accuracy: 1.00000\n",
      "Iteration: 22500 - cost: 0.00870 - accuracy: 1.00000\n",
      "Iteration: 22550 - cost: 0.00866 - accuracy: 1.00000\n",
      "Iteration: 22600 - cost: 0.00861 - accuracy: 1.00000\n",
      "Iteration: 22650 - cost: 0.00857 - accuracy: 1.00000\n",
      "Iteration: 22700 - cost: 0.00853 - accuracy: 1.00000\n",
      "Iteration: 22750 - cost: 0.00848 - accuracy: 1.00000\n",
      "Iteration: 22800 - cost: 0.00844 - accuracy: 1.00000\n",
      "Iteration: 22850 - cost: 0.00840 - accuracy: 1.00000\n",
      "Iteration: 22900 - cost: 0.00836 - accuracy: 1.00000\n",
      "Iteration: 22950 - cost: 0.00832 - accuracy: 1.00000\n",
      "Iteration: 23000 - cost: 0.00828 - accuracy: 1.00000\n",
      "Iteration: 23050 - cost: 0.00824 - accuracy: 1.00000\n",
      "Iteration: 23100 - cost: 0.00820 - accuracy: 1.00000\n",
      "Iteration: 23150 - cost: 0.00816 - accuracy: 1.00000\n",
      "Iteration: 23200 - cost: 0.00812 - accuracy: 1.00000\n",
      "Iteration: 23250 - cost: 0.00808 - accuracy: 1.00000\n",
      "Iteration: 23300 - cost: 0.00804 - accuracy: 1.00000\n",
      "Iteration: 23350 - cost: 0.00800 - accuracy: 1.00000\n",
      "Iteration: 23400 - cost: 0.00797 - accuracy: 1.00000\n",
      "Iteration: 23450 - cost: 0.00793 - accuracy: 1.00000\n",
      "Iteration: 23500 - cost: 0.00789 - accuracy: 1.00000\n",
      "Iteration: 23550 - cost: 0.00785 - accuracy: 1.00000\n",
      "Iteration: 23600 - cost: 0.00782 - accuracy: 1.00000\n",
      "Iteration: 23650 - cost: 0.00778 - accuracy: 1.00000\n",
      "Iteration: 23700 - cost: 0.00775 - accuracy: 1.00000\n",
      "Iteration: 23750 - cost: 0.00771 - accuracy: 1.00000\n",
      "Iteration: 23800 - cost: 0.00767 - accuracy: 1.00000\n",
      "Iteration: 23850 - cost: 0.00764 - accuracy: 1.00000\n",
      "Iteration: 23900 - cost: 0.00760 - accuracy: 1.00000\n",
      "Iteration: 23950 - cost: 0.00757 - accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24000 - cost: 0.00754 - accuracy: 1.00000\n",
      "Iteration: 24050 - cost: 0.00750 - accuracy: 1.00000\n",
      "Iteration: 24100 - cost: 0.00747 - accuracy: 1.00000\n",
      "Iteration: 24150 - cost: 0.00743 - accuracy: 1.00000\n",
      "Iteration: 24200 - cost: 0.00740 - accuracy: 1.00000\n",
      "Iteration: 24250 - cost: 0.00737 - accuracy: 1.00000\n",
      "Iteration: 24300 - cost: 0.00734 - accuracy: 1.00000\n",
      "Iteration: 24350 - cost: 0.00730 - accuracy: 1.00000\n",
      "Iteration: 24400 - cost: 0.00727 - accuracy: 1.00000\n",
      "Iteration: 24450 - cost: 0.00724 - accuracy: 1.00000\n",
      "Iteration: 24500 - cost: 0.00721 - accuracy: 1.00000\n",
      "Iteration: 24550 - cost: 0.00718 - accuracy: 1.00000\n",
      "Iteration: 24600 - cost: 0.00715 - accuracy: 1.00000\n",
      "Iteration: 24650 - cost: 0.00711 - accuracy: 1.00000\n",
      "Iteration: 24700 - cost: 0.00708 - accuracy: 1.00000\n",
      "Iteration: 24750 - cost: 0.00705 - accuracy: 1.00000\n",
      "Iteration: 24800 - cost: 0.00702 - accuracy: 1.00000\n",
      "Iteration: 24850 - cost: 0.00699 - accuracy: 1.00000\n",
      "Iteration: 24900 - cost: 0.00696 - accuracy: 1.00000\n",
      "Iteration: 24950 - cost: 0.00693 - accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "params_values, cost_history = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 25000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat, _ = forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = np.arange(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba65bf51d0>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_epochs, cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
